{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f4de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, TimeDistributed, LSTM, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f72d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Hyperparameters and Configurations\n",
    "# ===================================\n",
    "SEQUENCE_LENGTH = 16       # Number of frames per video clip\n",
    "IMG_SIZE = (224, 224)      # Image dimensions for each frame (compatible with MobileNetV2)\n",
    "BATCH_SIZE = 8             # Batch size for training\n",
    "EPOCHS = 30                # Maximum number of training epochs\n",
    "LEARNING_RATE = 1e-4       # Learning rate for optimizer\n",
    "L2_REG = 1e-3              # L2 regularization factor\n",
    "AUGMENT = True             # Enable data augmentation in the training generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e80912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 1. Load File Paths from Folders\n",
    "# ===================================\n",
    "# Directories for Accident and NoAccident videos (already divided)\n",
    "accident_dir = \"ProcessedDataset/Accident\"      # Accident videos folder\n",
    "noaccident_dir = \"ProcessedDataset/NoAccident\"    # NoAccident videos folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4508de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of video files (adjust file extensions as needed)\n",
    "accident_files = [os.path.join(accident_dir, f) for f in os.listdir(accident_dir)\n",
    "                  if f.lower().endswith(('.mp4', '.avi', '.mov'))]\n",
    "noaccident_files = [os.path.join(noaccident_dir, f) for f in os.listdir(noaccident_dir)\n",
    "                    if f.lower().endswith(('.mp4', '.avi', '.mov'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1138afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Accident videos:\", len(accident_files))\n",
    "print(\"Total NoAccident videos:\", len(noaccident_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7badcc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined lists with labels (Accident = 1, NoAccident = 0)\n",
    "all_files = accident_files + noaccident_files\n",
    "all_labels = [1] * len(accident_files) + [0] * len(noaccident_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a589a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 2. Split Data into Train+Validation and Test Sets\n",
    "# ===================================\n",
    "# First, split the dataset into train_val (80%) and test (20%) sets.\n",
    "train_val_files, test_files, train_val_labels, test_labels = train_test_split(\n",
    "    all_files, all_labels, test_size=0.2, random_state=42, stratify=all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa57fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTotal for training+validation:\", len(train_val_files))\n",
    "print(\"Total for testing (unseen):\", len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1446b1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further split train_val into training (80% of train_val) and validation (20% of train_val)\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(\n",
    "    train_val_files, train_val_labels, test_size=0.2, random_state=42, stratify=train_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803da361",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBefore oversampling (Training set):\")\n",
    "print(\"  Train Accident videos:\", sum(1 for l in train_labels if l == 1))\n",
    "print(\"  Train NoAccident videos:\", sum(1 for l in train_labels if l == 0))\n",
    "print(\"Validation set:\")\n",
    "print(\"  Validation Accident videos:\", sum(1 for l in val_labels if l == 1))\n",
    "print(\"  Validation NoAccident videos:\", sum(1 for l in val_labels if l == 0))\n",
    "print(\"Test set:\")\n",
    "print(\"  Test Accident videos:\", sum(1 for l in test_labels if l == 1))\n",
    "print(\"  Test NoAccident videos:\", sum(1 for l in test_labels if l == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355b5cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 3. Oversample the Minority Class in the Training Set\n",
    "# ===================================\n",
    "# Separate training file paths by label\n",
    "train_accident_files = [f for f, l in zip(train_files, train_labels) if l == 1]\n",
    "train_noaccident_files = [f for f, l in zip(train_files, train_labels) if l == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdc6086",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_accident = len(train_accident_files)\n",
    "num_noaccident = len(train_noaccident_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a429270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample the minority (NoAccident) if needed\n",
    "if num_noaccident < num_accident:\n",
    "    extra_needed = num_accident - num_noaccident\n",
    "    extra_noaccident_files = np.random.choice(train_noaccident_files, size=extra_needed, replace=True).tolist()\n",
    "    train_files += extra_noaccident_files\n",
    "    train_labels += [0] * extra_needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e25b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the oversampled training set\n",
    "combined_train = list(zip(train_files, train_labels))\n",
    "random.shuffle(combined_train)\n",
    "train_files, train_labels = zip(*combined_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b851d0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"\\nAfter oversampling (Training set):\")\n",
    "print(\"  Train Accident videos:\", sum(1 for l in train_labels if l == 1))\n",
    "print(\"  Train NoAccident videos:\", sum(1 for l in train_labels if l == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9652c35a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 4. Define Data Augmentation & Data Generator\n",
    "# ===================================\n",
    "def augment_frames(frames):\n",
    "    \"\"\"\n",
    "    Apply random augmentations to a sequence of frames.\n",
    "    Example augmentations: random horizontal flip and brightness adjustment.\n",
    "    \"\"\"\n",
    "    # Random horizontal flip with probability 0.5\n",
    "    if random.random() < 0.5:\n",
    "        frames = np.flip(frames, axis=2)  # Flip along the width axis\n",
    "    # Random brightness adjustment\n",
    "    brightness_factor = random.uniform(0.8, 1.2)\n",
    "    frames = np.clip(frames * brightness_factor, 0, 1)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292fa7d7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_frames(video_path, num_frames=SEQUENCE_LENGTH, img_size=IMG_SIZE):\n",
    "    \"\"\"\n",
    "    Extracts 'num_frames' evenly spaced frames from the video.\n",
    "    Resizes each frame to 'img_size', converts to RGB, and normalizes pixel values.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    if total_frames < num_frames:\n",
    "        indices = list(range(total_frames)) + [total_frames - 1] * (num_frames - total_frames)\n",
    "    else:\n",
    "        indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    \n",
    "    frame_id = 0\n",
    "    ret = True\n",
    "    while ret:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_id in indices:\n",
    "            frame = cv2.resize(frame, img_size)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "        frame_id += 1\n",
    "    cap.release()\n",
    "    \n",
    "    # Pad with last frame if needed\n",
    "    while len(frames) < num_frames:\n",
    "        frames.append(frames[-1])\n",
    "    \n",
    "    frames = np.array(frames, dtype=\"float32\") / 255.0\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b3b5d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class VideoDataGenerator(Sequence):\n",
    "    \"\"\"\n",
    "    Keras Sequence generator that yields batches of video clips and labels.\n",
    "    For training, data augmentation can be applied.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_paths, labels, batch_size=BATCH_SIZE, sequence_length=SEQUENCE_LENGTH, \n",
    "                 augment=False, shuffle=True):\n",
    "        self.file_paths = list(file_paths)\n",
    "        self.labels = list(labels)\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.augment = augment\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.file_paths) / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_paths = self.file_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_labels = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_X = []\n",
    "        for file_path in batch_paths:\n",
    "            frames = extract_frames(file_path, num_frames=self.sequence_length, img_size=IMG_SIZE)\n",
    "            if self.augment:\n",
    "                frames = augment_frames(frames)\n",
    "            batch_X.append(frames)\n",
    "        batch_X = np.array(batch_X)  # Shape: (batch_size, sequence_length, height, width, channels)\n",
    "        batch_y = np.array(batch_labels)\n",
    "        return batch_X, batch_y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            combined = list(zip(self.file_paths, self.labels))\n",
    "            random.shuffle(combined)\n",
    "            self.file_paths, self.labels = zip(*combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c0278e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create generators: apply augmentation for training only\n",
    "train_gen = VideoDataGenerator(train_files, train_labels, batch_size=BATCH_SIZE, \n",
    "                               sequence_length=SEQUENCE_LENGTH, augment=AUGMENT, shuffle=True)\n",
    "val_gen = VideoDataGenerator(val_files, val_labels, batch_size=BATCH_SIZE, \n",
    "                             sequence_length=SEQUENCE_LENGTH, augment=False, shuffle=False)\n",
    "test_gen = VideoDataGenerator(test_files, test_labels, batch_size=BATCH_SIZE, \n",
    "                              sequence_length=SEQUENCE_LENGTH, augment=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58748ae8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 5. Build the CNN+LSTM Model with Regularization\n",
    "# ===================================\n",
    "def build_model(sequence_length=SEQUENCE_LENGTH, img_size=IMG_SIZE):\n",
    "    \"\"\"\n",
    "    Builds a CNN+LSTM model.\n",
    "    MobileNetV2 is used as the feature extractor (TimeDistributed),\n",
    "    followed by an LSTM layer and a Dense output layer.\n",
    "    L2 regularization is applied to reduce overfitting.\n",
    "    \"\"\"\n",
    "    input_shape = (sequence_length, img_size[0], img_size[1], 3)\n",
    "    video_input = Input(shape=input_shape)\n",
    "    \n",
    "    # Base model: MobileNetV2 (pre-trained on ImageNet)\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(img_size[0], img_size[1], 3))\n",
    "    base_model.trainable = False  # Freeze base model\n",
    "    \n",
    "    x = TimeDistributed(base_model)(video_input)\n",
    "    x = TimeDistributed(GlobalAveragePooling2D())(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    # LSTM layer with L2 regularization\n",
    "    x = LSTM(64, kernel_regularizer=l2(L2_REG), return_sequences=False)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    # Dense output layer with L2 regularization\n",
    "    output = Dense(1, activation='sigmoid', kernel_regularizer=l2(L2_REG))(x)\n",
    "    \n",
    "    model = Model(inputs=video_input, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d098f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the model\n",
    "model = build_model(sequence_length=SEQUENCE_LENGTH, img_size=IMG_SIZE)\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=LEARNING_RATE), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe20b552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 6. Train the Model with Early Stopping and Model Checkpoint\n",
    "# ===================================\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint(\"best_accident_detection_model.h5\", monitor='val_loss', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167f2fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_gen, validation_data=val_gen, epochs=EPOCHS, \n",
    "          callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0f19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 7. Evaluate on Unseen Test Data\n",
    "# ===================================\n",
    "loss, accuracy = model.evaluate(test_gen)\n",
    "print(f\"\\nTest Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c068c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model if needed\n",
    "model.save(\"final_accident_detection_model.h5\")\n",
    "print(\"Model saved as 'final_accident_detection_model.h5'\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
